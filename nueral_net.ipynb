{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training a Nueral Network to crack LPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Packages + Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create LPN Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LPNOracle:\n",
    "    def __init__(self, secret, error_rate):\n",
    "        self.secret = secret\n",
    "        self.dimension = len(secret)\n",
    "        self.error_rate = error_rate\n",
    "\n",
    "    def sample(self, n_amount):\n",
    "        # Create random matrix.\n",
    "        A = np.random.randint(0, 2, size=(n_amount, self.dimension))\n",
    "        # Add Bernoulli errors.\n",
    "        e = np.random.binomial(1, self.error_rate, n_amount)\n",
    "        # Compute the labels.\n",
    "        b = np.mod(A @ self.secret + e, 2)\n",
    "        return A, b, e\n",
    "    \n",
    "    def percolate(self, A, b, e):\n",
    "        \n",
    "        goods = np.where(e == 0)[0]\n",
    "        bads = np.where(e == 1)[0]\n",
    "        A_good = A[goods, :]\n",
    "        A_bad = A[bads, :]\n",
    "        b_good = b[goods]\n",
    "        b_bad = b[bads]\n",
    "        \n",
    "        return A_good, b_good, A_bad, b_bad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataset Class(To load the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.from_numpy(x).to(torch.float32)\n",
    "    self.y = torch.from_numpy(y).to(torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "  def __len__(self):\n",
    "    return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "p = 0.125\n",
    "dim = 16 #CHANGE THIS\n",
    "s = np.random.randint(0, 2, dim)\n",
    "lpn = LPNOracle(s, p)\n",
    "\n",
    "sample_size = 10000\n",
    "A, b, e = lpn.sample(sample_size)\n",
    "\n",
    "A_good, b_good, A_bad, b_bad = lpn.percolate(A,b,e)\n",
    "\n",
    "\n",
    "trainset = dataset(A, b)\n",
    "bad_trainset = dataset(A_bad, b_bad)\n",
    "good_trainset = dataset(A_good, b_good)\n",
    "print(trainset.__len__())\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "good_trainloader = DataLoader(good_trainset, batch_size=64,shuffle=False)\n",
    "bad_trainloader = DataLoader(bad_trainset, batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : [[1 1 1 ... 0 0 0]\n",
      " [0 1 0 ... 1 1 0]\n",
      " [0 1 1 ... 1 0 0]\n",
      " ...\n",
      " [1 0 1 ... 1 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 1 ... 0 1 0]]\n",
      "A shape: (10000, 16)\n",
      "b : [0 0 0 ... 1 0 1]\n",
      "B shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"A : {a}\".format(a = A))\n",
    "print(\"A shape: {a}\".format(a = A.shape))\n",
    "print(\"b : {b}\".format(b = b))\n",
    "print(\"B shape: {b}\".format(b = b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the Experiment Folder + Save the Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while os.path.exists(\"./experiment-{a}\".format(a = counter)):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "print(counter)\n",
    "    \n",
    "os.mkdir(\"./experiment-{a}\".format(a = counter))\n",
    "\n",
    "slist = s.tolist()\n",
    "A_list = A.tolist()\n",
    "b_list = b.tolist()\n",
    "s_dict = {\"s\" : slist, \"A\" : A_list, \"b\" : b_list, \"p\" : p}\n",
    "\n",
    "with open(\"./experiment-{a}/secret.json\".format(a = counter), \"w\") as sfile:\n",
    "    json.dump(s_dict, sfile)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Parametric.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parametric = {}\n",
    "\n",
    "parametric[\"layers\"] = [96];\n",
    "parametric[\"s\"] = len(s.tolist())\n",
    "\n",
    "with open(\"./experiment-{a}/parametric.json\".format(a = counter), \"w\") as file:\n",
    "    \n",
    "    json.dump(parametric, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class cryptnet(nn.Module):\n",
    "  def __init__(self,s):\n",
    "    super(cryptnet,self).__init__()\n",
    "    self.fc1 = nn.Linear(s.size,32)\n",
    "    self.act1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(32,32)\n",
    "    self.act2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(32,1)\n",
    "    self.siggy = nn.Sigmoid()\n",
    "  def forward(self,x):\n",
    "    x = self.act1(self.fc1(x))\n",
    "    x = self.act2(self.fc2(x))\n",
    "    x = self.siggy(self.fc3(x))\n",
    "    return x\n",
    "\n",
    "chicken_net = cryptnet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class parametric_cryptnet(nn.Module):\n",
    "    def __init__(self, parametric):\n",
    "        super(parametric_cryptnet, self).__init__()\n",
    "        self.act = nn.ModuleList()\n",
    "        self.fc = nn.ModuleList()\n",
    "    \n",
    "        layer_arr = parametric[\"layers\"]            \n",
    "        size_lay = len(layer_arr)\n",
    "    \n",
    "        self.fc.append(nn.Linear(parametric[\"s\"], layer_arr[0]))\n",
    "        self.act.append(nn.ReLU())\n",
    "    \n",
    "        for i in range(size_lay - 1):\n",
    "            self.fc.append(nn.Linear(layer_arr[i], layer_arr[i + 1]))\n",
    "            self.act.append(nn.ReLU())\n",
    "                       \n",
    "        self.fc.append(nn.Linear(layer_arr[size_lay - 1], 1))\n",
    "        self.act.append(nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i in range(0, len(self.fc)):\n",
    "            x = self.act[i](self.fc[i](x))\n",
    "        return x\n",
    "\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.5: Training a Model on the Good Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s  : [1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0]\n",
      "s' : [1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1]\n",
      "epoch 0\tloss : 0.6817697882652283\t accuracy : 0.512206251425964\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output,y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Calculation of accuracy and other things\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mparacrypt_good\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgood_trainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m acc \u001b[38;5;241m=\u001b[39m (predicted\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround() \u001b[38;5;241m==\u001b[39m b_good)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#backprop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [69], line 24\u001b[0m, in \u001b[0;36mparametric_cryptnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc)):\n\u001b[0;32m---> 24\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/activation.py:295\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paracrypt_good = parametric_cryptnet(parametric)\n",
    "learning_rate = 0.01\n",
    "epochs = 1600\n",
    "optimizer = torch.optim.SGD(paracrypt_good.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(\"s  : {a}\".format(a = s))\n",
    "\n",
    "with open(\"./experiment-{a}/good_training.txt\".format(a = counter), \"w\") as data_file:\n",
    "    for i in range(epochs):\n",
    "        for j,(x_train,y_train) in enumerate(good_trainloader):\n",
    "\n",
    "            #calculate output\n",
    "            output = paracrypt_good(x_train)\n",
    "\n",
    "            #calculate loss\n",
    "            loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "\n",
    "            #Calculation of accuracy and other things\n",
    "            predicted = paracrypt_good(good_trainset.x)\n",
    "            acc = (predicted.reshape(-1).detach().numpy().round() == b_good).mean()\n",
    "\n",
    "\n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            losses.append(loss)\n",
    "            accur.append(acc)\n",
    "            \n",
    "            paracrypt_good.eval()\n",
    "            \n",
    "            trick = torch.from_numpy(np.eye(dim)).to(torch.float32)\n",
    "            s_candidate = paracrypt_good(trick).reshape(-1).detach().numpy().round()\n",
    "            print(\"s' : {b}\".format(b = s_candidate.astype(np.int16)))\n",
    "            \n",
    "            paracrypt_good.train()\n",
    "            \n",
    "            print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))\n",
    "            data_file.write(\"epoch {}\\tloss : {}\\t accuracy : {} \\n\".format(i,loss,acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(paracrypt_good.state_dict(), \"./experiment-{a}/good_paracrypt.pth\".format(a = counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Starting the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\tloss : 0.6966257095336914\t accuracy : 0.5056 good_acc : 0.5095824777549623 , bad_acc : 0.49432739059967584\n",
      "\n",
      "epoch 50\tloss : 0.6932664513587952\t accuracy : 0.5244 good_acc : 0.526694045174538 , bad_acc : 0.5210696920583469\n",
      "\n",
      "epoch 100\tloss : 0.6892038583755493\t accuracy : 0.5342 good_acc : 0.539812913529546 , bad_acc : 0.5332252836304701\n",
      "\n",
      "epoch 150\tloss : 0.6815876960754395\t accuracy : 0.5455 good_acc : 0.5467716176135067 , bad_acc : 0.5364667747163695\n",
      "\n",
      "epoch 200\tloss : 0.6714800596237183\t accuracy : 0.5499 good_acc : 0.5529317818845539 , bad_acc : 0.5453808752025932\n",
      "\n",
      "epoch 250\tloss : 0.6598396301269531\t accuracy : 0.5582 good_acc : 0.5597764088523842 , bad_acc : 0.5453808752025932\n",
      "\n",
      "epoch 300\tloss : 0.647910475730896\t accuracy : 0.5653 good_acc : 0.5660506502395619 , bad_acc : 0.5470016207455429\n",
      "\n",
      "epoch 350\tloss : 0.6370813846588135\t accuracy : 0.5715 good_acc : 0.5726671229751312 , bad_acc : 0.5567260940032415\n",
      "\n",
      "epoch 400\tloss : 0.6257693767547607\t accuracy : 0.5774 good_acc : 0.582363677846224 , bad_acc : 0.5502431118314425\n",
      "\n",
      "epoch 450\tloss : 0.6111692786216736\t accuracy : 0.5821 good_acc : 0.5879534565366188 , bad_acc : 0.5575364667747164\n",
      "\n",
      "epoch 500\tloss : 0.5987201929092407\t accuracy : 0.5896 good_acc : 0.5918320784850559 , bad_acc : 0.5632090761750406\n",
      "\n",
      "epoch 550\tloss : 0.5851923823356628\t accuracy : 0.5945 good_acc : 0.6020990189368013 , bad_acc : 0.5559157212317666\n",
      "\n",
      "epoch 600\tloss : 0.5692319273948669\t accuracy : 0.6012 good_acc : 0.6057494866529775 , bad_acc : 0.5421393841166937\n",
      "\n",
      "epoch 650\tloss : 0.5561017990112305\t accuracy : 0.6052 good_acc : 0.6151038101756787 , bad_acc : 0.5413290113452188\n",
      "\n",
      "epoch 700\tloss : 0.5479442477226257\t accuracy : 0.6102 good_acc : 0.6228610540725531 , bad_acc : 0.5356564019448946\n",
      "\n",
      "epoch 750\tloss : 0.5329129099845886\t accuracy : 0.6188 good_acc : 0.6352954597307781 , bad_acc : 0.5210696920583469\n",
      "\n",
      "epoch 800\tloss : 0.5087701678276062\t accuracy : 0.6378 good_acc : 0.6566278804471823 , bad_acc : 0.5105348460291734\n",
      "\n",
      "epoch 850\tloss : 0.49055215716362\t accuracy : 0.6569 good_acc : 0.6833219256217202 , bad_acc : 0.4846029173419773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paracrypt = parametric_cryptnet(parametric)\n",
    "learning_rate = 0.01\n",
    "epochs = 900\n",
    "optimizer = torch.optim.SGD(paracrypt.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "accur = []\n",
    "losses = []\n",
    "\n",
    "with open(\"./experiment-{a}/training.txt\".format(a = counter), \"w\") as data_file:\n",
    "    for i in range(epochs):\n",
    "        for j,(x_train,y_train) in enumerate(trainloader):\n",
    "\n",
    "            #calculate output\n",
    "            output = paracrypt(x_train)\n",
    "\n",
    "            #calculate loss\n",
    "            loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "\n",
    "            #Calculation of accuracy and other things\n",
    "            predicted = paracrypt(trainset.x)\n",
    "            acc = (predicted.reshape(-1).detach().numpy().round() == b).mean()\n",
    "\n",
    "\n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            losses.append(loss)\n",
    "            accur.append(acc)\n",
    "            pred_good = paracrypt(good_trainset.x)\n",
    "            acc_good = (pred_good.reshape(-1).detach().numpy().round() == b_good).mean()\n",
    "            pred_bad = paracrypt(bad_trainset.x)\n",
    "            acc_bad = (pred_bad.reshape(-1).detach().numpy().round() == b_bad).mean()\n",
    "            print(\"epoch {}\\tloss : {}\\t accuracy : {} good_acc : {} , bad_acc : {}\\n\".format(i,loss,acc, acc_good, acc_bad))\n",
    "            data_file.write(\"epoch {}\\tloss : {}\\t accuracy : {} good_acc : {} , bad_acc : {}\\n\".format(i,loss,acc, acc_good, acc_bad))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.5: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(paracrypt.state_dict(), \"./experiment-{a}/paracrypt.pth\".format(a = counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\tloss : 0.216057687997818\t accuracy : 0.8766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [115], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./experiment-\u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m/training.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(a \u001b[38;5;241m=\u001b[39m counter), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m data_file:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j,(x_train,y_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m             \u001b[38;5;66;03m#calculate output\u001b[39;00m\n\u001b[1;32m     18\u001b[0m             output \u001b[38;5;241m=\u001b[39m paracrypt(x_train)\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;66;03m#calculate loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [105], line 11\u001b[0m, in \u001b[0;36mdataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_bad \u001b[38;5;241m=\u001b[39m b_bad\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paracrypt = parametric_cryptnet(parametric)\n",
    "paracrypt.load_state_dict(torch.load(\"./experiment-{a}/paracrypt.pth\".format(a = counter)))\n",
    "paracrypt.train()\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 400\n",
    "optimizer = torch.optim.SGD(paracrypt.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "accur = []\n",
    "losses = []\n",
    "\n",
    "with open(\"./experiment-{a}/training.txt\".format(a = counter), \"w\") as data_file:\n",
    "    for i in range(epochs):\n",
    "        for j,(x_train,y_train) in enumerate(trainloader):\n",
    "\n",
    "            #calculate output\n",
    "            output = paracrypt(x_train)\n",
    "\n",
    "            #calculate loss\n",
    "            loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "\n",
    "            #Calculation of accuracy and other things\n",
    "            predicted = paracrypt(trainset.x)\n",
    "            acc = (predicted.reshape(-1).detach().numpy().round() == b).mean()\n",
    "\n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i%50 == 0:\n",
    "            losses.append(loss)\n",
    "            accur.append(acc)\n",
    "            print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))\n",
    "            data_file.write(\"epoch {}\\tloss : {}\\t accuracy : {}\\n\".format(i,loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Evaluating the Accuracy of the Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_num = 32 #PLEASE CHANGE THIS VALUE|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s' : [0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0]\n",
      "s  : [0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#We need to load from A, b, and blah blah based on the experiment number\n",
    "trick = torch.from_numpy(np.eye(dim)).to(torch.float32)\n",
    "\n",
    "import json\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "with open(\"./experiment-{a}/secret.json\".format(a = experiment_num), \"r\") as data_file:\n",
    "    data_dict = json.load(data_file)\n",
    "    \n",
    "A = np.array(data_dict[\"A\"])\n",
    "b = np.array(data_dict[\"b\"])\n",
    "s = np.array(data_dict[\"s\"])\n",
    "p = data_dict[\"p\"]\n",
    "\n",
    "parametric = {}\n",
    "\n",
    "with open(\"./experiment-{a}/parametric.json\".format(a = experiment_num), \"r\") as data_file:\n",
    "    parametric = json.load(data_file)\n",
    "    \n",
    "k = parametric[\"s\"]\n",
    "\n",
    "paracrypt = parametric_cryptnet(parametric)\n",
    "paracrypt.load_state_dict(torch.load(\"./experiment-{a}/paracrypt.pth\".format(a = experiment_num))) #CHANGE THIS\n",
    "paracrypt.eval()\n",
    "\n",
    "s_candidate = paracrypt(trick).reshape(-1).detach().numpy().round()\n",
    "\n",
    "print(\"s' : {b}\".format(b = s_candidate.astype(np.int16)))\n",
    "print(\"s  : {a}\".format(a = s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experiment-32/good_paracrypt.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [50], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m k \u001b[38;5;241m=\u001b[39m parametric[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m paracrypt_good \u001b[38;5;241m=\u001b[39m parametric_cryptnet(parametric)\n\u001b[0;32m---> 24\u001b[0m paracrypt\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./experiment-\u001b[39;49m\u001b[38;5;132;43;01m{a}\u001b[39;49;00m\u001b[38;5;124;43m/good_paracrypt.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexperiment_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#CHANGE THIS\u001b[39;00m\n\u001b[1;32m     25\u001b[0m paracrypt\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m s_candidate \u001b[38;5;241m=\u001b[39m paracrypt(trick)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mround()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './experiment-32/good_paracrypt.pth'"
     ]
    }
   ],
   "source": [
    "#We need to load from A, b, and blah blah based on the experiment number\n",
    "trick = torch.from_numpy(np.eye(dim)).to(torch.float32)\n",
    "\n",
    "import json\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "with open(\"./experiment-{a}/secret.json\".format(a = experiment_num), \"r\") as data_file:\n",
    "    data_dict = json.load(data_file)\n",
    "    \n",
    "A = np.array(data_dict[\"A\"])\n",
    "b = np.array(data_dict[\"b\"])\n",
    "s = np.array(data_dict[\"s\"])\n",
    "p = data_dict[\"p\"]\n",
    "\n",
    "parametric = {}\n",
    "\n",
    "with open(\"./experiment-{a}/parametric.json\".format(a = experiment_num), \"r\") as data_file:\n",
    "    parametric = json.load(data_file)\n",
    "    \n",
    "k = parametric[\"s\"]\n",
    "\n",
    "paracrypt_good = parametric_cryptnet(parametric)\n",
    "paracrypt.load_state_dict(torch.load(\"./experiment-{a}/good_paracrypt.pth\".format(a = experiment_num))) #CHANGE THIS\n",
    "paracrypt.eval()\n",
    "\n",
    "s_candidate = paracrypt(trick).reshape(-1).detach().numpy().round()\n",
    "\n",
    "print(\"s' : {b}\".format(b = s_candidate.astype(np.int16)))\n",
    "print(\"s  : {a}\".format(a = s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming weight : 1234.0\n",
      "Prediction was correct!\n"
     ]
    }
   ],
   "source": [
    "#Calculate the Hamming Weight\n",
    "import math\n",
    "hamming_weight = np.mod(A @ s_candidate + b, 2).sum()\n",
    "print(\"hamming weight : {a}\".format(a = hamming_weight))\n",
    "\n",
    "#Need to figure out the formula to calculate the bounds tomorrow, and make this even more\n",
    "#parametric to work faster and figure out more\n",
    "\n",
    "m = 4 * k * math.pow(0.5 - p, -2)\n",
    "bound = m * p + math.pow(k * m, 0.5)\n",
    "\n",
    "with open(\"./experiment-{a}/results.txt\".format(a = experiment_num), \"w\") as data_file:\n",
    "    if hamming_weight < sample_size * p + bound:\n",
    "        print(\"Prediction was correct!\")\n",
    "        data_file.write(\"Prediction was correct!\\n\")\n",
    "    else:\n",
    "        print('Wrong candidate. Try again!')\n",
    "        data_file.write('Wrong candidate. Try again!\\n')\n",
    "        \n",
    "    data_file.write(\"s' : {b}\\n\".format(b = s_candidate))\n",
    "    data_file.write(\"s : {a}\\n\".format(a = s))\n",
    "    data_file.write(\"hamming weight : {a}\\n\".format(a = hamming_weight))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Delete all the models(God forbid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./experiment-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 1 0 ... 0 1 0]\n",
      " ...\n",
      " [1 0 1 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 1 0]]\n",
      "[  0   1   2   4   5   6   7   8   9  11  12  13  14  15  17  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  39  40\n",
      "  41  42  44  45  46  47  48  49  50  51  53  54  57  58  59  60  61  62\n",
      "  63  65  66  67  69  70  71  73  74  75  76  77  78  80  81  82  83  84\n",
      "  85  87  88  89  90  91  92  93  94  95  96  97  98  99 100 102 103 104\n",
      " 105 106 108 109 110 111 112 113 115 116 117 118 119 120 121 122 123 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 168 169 170 171 172 174 175 176 178 179 180 182 183\n",
      " 184 185 186 187 188 190 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 216 217 218 219 220 221 222\n",
      " 223 224 225 226 227 228 230 231 232 233 234 235 236 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 252 253 254 255 256 257 258 260 261 262\n",
      " 263 264 266 267 269 270 271 272 273 274 275 276 277 278 279 280 281 282\n",
      " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 303 304 305 306 308 309 310 311 312 314 315 316 317 318 319 320 321\n",
      " 322 324 325 326 327 328 329 330 332 333 334 335 336 337 338 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 354 356 357 358 359 360 361 362\n",
      " 363 365 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 383\n",
      " 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 407 408 409 411 412 413 414 415 416 417 419 421 422 423\n",
      " 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441\n",
      " 442 443 444 445 447 448 449 450 451 452 453 454 456 457 458 459 460 461\n",
      " 462 463 464 465 466 467 468 469 470 471 472 473 475 476 477 478 479 480\n",
      " 481 482 483 484 485 486 487 488 489 490 491 493 494 495 496 497 498 499\n",
      " 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517\n",
      " 518 519 520 521 522 523 524 526 527 528 529 530 531 532 534 535 536 537\n",
      " 539 541 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 566 567 568 569 570 571 572 573 575 576 577 578 579\n",
      " 580 581 582 584 585 586 587 588 589 590 591 592 593 594 596 597 598 599\n",
      " 600 601 603 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619\n",
      " 620 623 624 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640\n",
      " 641 642 645 646 647 650 651 652 653 654 655 656 657 658 659 661 663 664\n",
      " 665 666 667 668 669 670 671 672 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 694 695 696 697 698 699 700 701 702\n",
      " 703 705 706 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723\n",
      " 724 725 726 727 729 730 731 732 733 734 735 736 737 738 739 740 741 742\n",
      " 743 744 745 746 747 748 749 751 752 753 754 755 756 758 759 760 761 762\n",
      " 763 764 765 766 767 769 770 771 772 773 774 775 776 777 778 779 782 783\n",
      " 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801\n",
      " 803 804 805 806 807 808 811 812 813 814 816 818 821 823 824 825 826 827\n",
      " 828 829 830 831 832 833 834 835 836 838 839 840 841 843 844 845 846 848\n",
      " 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866\n",
      " 867 869 870 871 872 873 875 876 878 880 884 885 886 887 890 891 893 894\n",
      " 895 896 897 898 899 900 901 902 903 904 905 906 907 910 911 912 913 914\n",
      " 915 916 917 920 921 922 923 924 926 928 929 930 931 932 933 934 935 936\n",
      " 937 939 940 941 942 943 944 945 946 947 948 949 950 951 952 954 955 956\n",
      " 957 958 959 960 961 962 963 964 965 966 967 969 970 972 974 975 976 977\n",
      " 978 979 980 982 983 984 985 986 987 988 990 991 992 993 994 995 996 997\n",
      " 998 999]\n",
      "[  3  10  16  18  38  43  52  55  56  64  68  72  79  86 101 107 114 124\n",
      " 167 173 177 181 189 191 215 229 237 251 259 265 268 283 307 313 323 331\n",
      " 339 353 355 364 366 382 406 410 418 420 446 455 474 492 525 533 538 540\n",
      " 542 564 565 574 583 595 602 604 621 622 625 643 644 648 649 660 662 673\n",
      " 693 704 707 708 728 750 757 768 780 781 802 809 810 815 817 819 820 822\n",
      " 837 842 847 868 874 877 879 881 882 883 888 889 892 908 909 918 919 925\n",
      " 927 938 953 968 971 973 981 989]\n",
      "(884, 32)\n",
      "(116, 32)\n",
      "[0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1\n",
      " 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
      " 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
      " 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0\n",
      " 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
      " 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0\n",
      " 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
      " 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
      " 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
      " 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
      " 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "##Test bench code\n",
    "import numpy as np\n",
    "e = np.random.binomial(1, 0.125, 1000)\n",
    "A = np.random.randint(0, 2, size=(1000, 32))\n",
    "\n",
    "print(A)\n",
    "goods = np.where(e == 0)[0]\n",
    "bads = np.where(e == 1)[0]\n",
    "print(goods)\n",
    "print(bads)\n",
    "\n",
    "A_good = A[goods, :]\n",
    "print(A_good.shape)\n",
    "\n",
    "A_bad = A[bads, :]\n",
    "print(A_bad.shape)\n",
    "\n",
    "b_good = b[goods]\n",
    "print(b_good)\n",
    "\n",
    "##So this code actually works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
